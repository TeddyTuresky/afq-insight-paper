<section data-menu-title="Methods: Statistical Learning and Group Structure">
  <h2>Methods: Statistical Learning and Group Structure</h2>
  <div class="row">
    <div class="col-23">
      <img class="logo" style="padding: 30px;" height="400" align="middle"
        src="img/dmri-group-structure.svg"
        alt="Group structure of dMRI data">
      <img class="logo" style="padding: 30px;" height="400" align="middle"
        src="img/afq-insight-nested-cross-validation.svg"
        alt="Hyperparameter optimization through nested cross-validation">
      <script type="math/tex; mode=display">
        \widehat{y} = \mathbf{X} \cdot \widehat{\beta}, \\
            \widehat{\beta} = \min_\beta
            \left[ \left\lVert \widehat{y} - \mathbf{X} \cdot \beta \right\lVert_2^2
            + \lambda_1 \displaystyle \sum_\ell \sqrt{p_\ell}
            \left\lVert \beta^{(\ell)} \right\lVert_2
            + \lambda_2 \left\lVert \beta \right\lVert_1 \right]
      </script>
    </div>
    <div class="col-13">
      <ul>
        <li>Diffusion MRI data has inherent group structure</li>
        <li>Sparse group lasso (SGL) exploits this structure.</li>
        <li>Hyperparameters are chosen through nested cross-validation.</li>
        <li>Results are sparse and identify important white matter features</li>
      </ul>
    </div>
  </div>
  <aside class="notes" data-markdown>
    - Having described WM
    - I can explain this new approach that I mentioned in the beginning
  </aside>
</section>

<section>
  <h3>Regression framework</h3>
  <ul>
    <li>Want to both predict diagnoses and interpret features.</li>
    <li>
      Regression perspective: solve for <script type="math/tex; mode=inline">\beta</script> in
      <script type="math/tex; mode=display">
        \widehat{y} = \mathbf{X} \cdot \beta
      </script>
    </li>
    <ul>
      <li>
        Features <script type="math/tex; mode=inline">\mathbf{X} \in \mathbb{R}^{n \times p}</script>:
        Individual nodes in each white matter tract
      </li>
      <li>
        Response <script type="math/tex; mode=inline">y</script>:
        Diagnosis
      </li>
      <li>
        Relationship characterized by <script type="math/tex; mode=inline">\beta</script>
      </li>
    </ul>
  </ul>
  <aside class="notes" data-markdown>
    - Want to make predictions and discover informative features
    - Calls for regression perspective
    - Interest is on conditional distribution of response variable, given set of features
    - Important to characterize how features are related to response
    - As opposed to automated translation or image classification
  </aside>
</section>

<section>
  <h3>Group structure</h3>
  <ul>
    <li>dMRI data has has group structure</li>
  </ul>
  <img class="logo" style="padding: 30px;" height="700" align="middle"
    src="img/dmri-group-structure.svg"
    alt="Group structure of dMRI data">
  <aside class="notes" data-markdown>
    - So that's why we used regression
    - But, dMRI has group structure too
    - We should use an algorithm that can capitalize on this information
  </aside>
</section>

<section>
  <h3>Regularization approaches</h3>
  <ul>
    <li> Need regularization because
      <script type="math/tex; mode=inline">p \gg n.</script>
    </li>
    <li>The Lasso:
      <div class="eqcite">
        <script type="math/tex; mode=display">
          \left\lVert \widehat{y} - \mathbf{X} \cdot \beta \right\lVert_2^2
          + \lambda_2 \left\lVert \beta \right\lVert_1
        </script>
        <cite>
          <a href="http://statweb.stanford.edu/~tibs/lasso/lasso.pdf">Tibshirani (1996), J. R. Statist. Soc. B</a>
        </cite>
      </div>
      <ul>
        <li>Global sparsity</li>
        <li>Ignores group structure</li>
      </ul>
    </li>
    <li>The Group Lasso:
      <div class="eqcite">
        <script type="math/tex; mode=display">
          \left\lVert \widehat{y} - \mathbf{X} \cdot \beta \right\lVert_2^2
          + \lambda_1 \displaystyle \sum_\ell \sqrt{p_\ell}
          \left\lVert \beta^{(\ell)} \right\lVert_2
        </script>
        <cite>
          <a href="https://doi.org/10.1111/j.1467-9868.2005.00532.x">Yaun and Lin (2006), J. R. Statist. Soc. B</a>
        </cite>
      </div>
      <ul>
        <li><script type="math/tex; mode=inline">\ell</script>:
          number of groups 
        </li>
        <li><script type="math/tex; mode=inline">p(\ell)</script>:
          group size
        </li>
        <li>Enforces inter-group sparsity</li>
        <li>Lacks intra-group sparsity</li>
      </ul>
    </li>
  </ul>
  <aside class="notes" data-markdown>
    - But we have way too many features
    - Need to regularize
    - Naive regularization strategies don't cut it
    - Lasso alone yields 70% accuracy and doesn't identify correct features
  </aside>
</section>

<section>
  <h3>Sparse Group Lasso</h3>
  <ul>
    <li>Minimize
      <div class="eqcite">
        <script type="math/tex; mode=display">
          \left\lVert \widehat{y} - \mathbf{X} \cdot \beta \right\lVert_2^2
          + \lambda_1 \displaystyle \sum_\ell \sqrt{p_\ell}
          \left\lVert \beta^{(\ell)} \right\lVert_2
          + \lambda_2 \left\lVert \beta \right\lVert_1
        </script>
        <cite>
          <a href="https://doi.org/10.1080/10618600.2012.681250">Simon et al. (2013), J. Computational and Graphical Statistics</a>
        </cite>
      </div>
    </li>
    <li>Subsumes Lasso
      <script type="math/tex; mode=inline">(\lambda_1 = 0)</script>
    </li>
    <li>And Group Lasso
      <script type="math/tex; mode=inline">(\lambda_2 = 0)</script>
    </li>
    <li>Enforces both intra-group and inter-group sparsity</li>
    <li>The price is one additional hyperparameter.</li>
  </ul>
  <aside class="notes" data-markdown>
    - Having identified the group structure...
    - Noah (SGL - 2013)
    - Intra-group and inter-group sparsity
    - Feed in all groups
    - Optimize one more hyperparameter
  </aside>
</section>

<section>
  <h3>Hyperparameter optimization</h3>
  <p style="text-align: left;">
    <script type="math/tex; mode=inline">
      \alpha_1, \alpha_2
    </script>
    are determined using nested K-fold cross-validation (CV).
  </p>
  <img class="logo" style="padding: 30px;" width="800" align="middle"
    src="img/afq-insight-nested-cross-validation.svg"
    alt="Hyperparameter optimization through nested cross-validation">
  <p style="text-align: left;">
    The inner CV loop selects the best hyperparameters. The outer CV
    loop evaluates the model. Each loop uses repeated K-fold CV and
    the folds are stratified by class in the classification case.
  </p>
  <aside class="notes" data-markdown>
    - Inner CV loop selects the right hyperparameters
    - Outer CV loop evaluates the model
    - Using repeated K-fold for each loop
    - Stratified K-fold if classification problem
  </aside>
</section>
